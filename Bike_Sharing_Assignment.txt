#!/usr/bin/env python
# coding: utf-8

# 
# ## Mulitple Linear Regression
# ### Problem Statement
# A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.
# 
# They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:
# 
# - Which variables are significant in predicting the demand for shared bikes.
# - How well those variables describe the bike demands

# In[1]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score,mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

import warnings
warnings.filterwarnings('ignore')
get_ipython().run_line_magic('matplotlib', 'inline')


# ## Reading and Understanding Data

# In[2]:


bike_df = pd.read_csv('day.csv')
#reading the data
bike_df


# ### Dataset has 730 rows and 16 columns

# In[3]:


bike_df.shape #checking the size of the data


# In[4]:


bike_df.info() #checking out for detailed info about the dataset
#Checking for count and the type of data present in the given dataset


# In[5]:


bike_df.describe()


# In[6]:


bike_df.size #describes the total size of the dataset


# ## Data Cleaning

# ### Drop columns that are nt useful for data analysis
# - instant:as it is the record index
# - dteday: as the features of date are alreeady there like yr month and weekday
# - casual and registered as the are in cnt, because cnt is sum of both the values

# In[7]:


bike_df.isnull().sum() #checking for missing values


# In[8]:


bike_df.drop(['instant'],axis=1,inplace=True)
#dropping instant column as it is merely a index column which has no significance for our target


# In[9]:


bike_df.drop(['dteday'],axis=1,inplace=True)
#dteday is not useful as month and weekday are covering it

bike_df.drop(['casual'],axis=1,inplace=True)
bike_df.drop(['registered'],axis=1,inplace=True)
#Removing casual and registered as cnt is sum of these


#  Inspecting data after dropping 

# In[10]:


bike_df.head()


# In[11]:


bike_df.info()


# In[12]:


bike_df.columns #identifying all the column head


# In[13]:


bike_df.corr()


# So from the above correlation formed we can clearly see that season,month and weekday which are supposed 
# to be non-numerical are numerical which should be changed 
# 

# ### Handling outliers

# In[14]:


bike_df.columns


# In[15]:


bike_df.nunique() #gives info about unique values present in data


# In[16]:


cols = ['temp', 'atemp', 'hum', 'windspeed']
plt.figure(figsize=(18,4))

i = 1
for col in cols:
    plt.subplot(1,4,i)
    sns.boxplot(y=col, data=bike_df)
    i+=1


# So from the above plots it is clear that we don't have any outliers

# ## EDA

# In[17]:


#changing categorical data which were primarily numeric to more meaningful one
bike_df.season.replace({1:"spring", 2:"summer", 3:"fall", 4:"winter"},inplace = True)

bike_df.weathersit.replace({1:'good',2:'moderate',3:'bad',4:'severe'},inplace = True)

bike_df.mnth = bike_df.mnth.replace({1: 'jan',2: 'feb',3: 'mar',4: 'apr',5: 'may',6: 'jun',
                  7: 'jul',8: 'aug',9: 'sept',10: 'oct',11: 'nov',12: 'dec'})

bike_df.weekday = bike_df.weekday.replace({0: 'sun',1: 'mon',2: 'tue',3: 'wed',4: 'thu',5: 'fri',6: 'sat'})
bike_df.head()


# ### Drawing pairplots to check for linear relationship

# In[18]:


plt.figure(figsize = (15,30))
sns.pairplot(data=bike_df,vars=['cnt', 'temp', 'atemp', 'hum','windspeed'])
plt.show()


# - So from the above plots we can clearly understand that temp and atemp are having high correlation
# - And from the plots we can also say that there is alinear relationship between TEMP and ATEMP

# ### Visualising data to find correlation from numerical variables

# In[19]:


plt.figure(figsize=(20,15))
sns.pairplot(bike_df)
plt.show()


# ### Heatmap for correlation between numeric variables

# In[20]:


numeric_bike_df = bike_df.select_dtypes(include=[np.number])


# In[21]:


plt.figure(figsize=(15,10))
sns.heatmap(numeric_bike_df.corr(),cmap="YlGnBu",annot=True)
plt.show()


# In[22]:


#visualising the categorical variables

plt.figure(figsize=(20,15))

plt.subplot(2,3,1)
sns.boxplot(x='season',y='cnt',data=bike_df)

plt.subplot(2,3,2)
sns.boxplot(x='yr',y='cnt',data=bike_df)

plt.subplot(2,3,3)
sns.boxplot(x='mnth',y='cnt',data=bike_df)

plt.subplot(2,3,4)
sns.boxplot(x='weekday',y='cnt',data=bike_df)

plt.subplot(2,3,5)
sns.boxplot(x='workingday',y='cnt',data=bike_df)

plt.subplot(2,3,6)
sns.boxplot(x='weathersit',y='cnt',data=bike_df)


# In[23]:


num_features = ["temp","atemp","hum","windspeed","cnt"]
plt.figure(figsize=(15,8),dpi=130)
plt.title("Correlation betweeen numeric features",fontsize=16)
sns.heatmap(bike_df[num_features].corr(),annot= True,cmap="mako")
plt.show()


# In[24]:


bike_df.describe()


# ## Data preparation for linar regression

# Creating dummy variables for categorical variables

# In[25]:


bike_df = pd.get_dummies(data=bike_df,columns=["season","mnth","weekday"],drop_first=True)
bike_df = pd.get_dummies(data=bike_df,columns=["weathersit"])


# In[26]:


bike_df.columns


# In[27]:


bike_df.head()


# ### Splitting data into train and test data

# In[28]:


bike_df.shape


# In[29]:


#y to contain only target variable
y=bike_df.pop('cnt')

#X is all remainign variable also our independent variables
X=bike_df

#Train Test split with 70:30 ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# In[30]:


X.head()


# In[31]:


# Checking shape and size for train and test
print(X_train.shape)
print(X_test.shape)


# In[32]:


# Let us scale continuous variables
num_vars = ['temp','atemp','hum','windspeed']

#Use Normalized scaler to scale
scaler = MinMaxScaler()

#Fit and transform training set only
X_train[num_vars] = scaler.fit_transform(X_train[num_vars])


# In[33]:


X_train.describe()


# In[34]:


X_train.head()


# In[35]:


plt.figure(figsize = (15, 15))  #Checking if the variables are highly correlated
sns.heatmap(X_train.corr(), annot = True)
plt.show()


# ### Build model using RFE

# In[36]:


lr = LinearRegression()
lr.fit(X_train,y_train)


# In[37]:


#Cut down number of features to 15 using automated approach

rfe = RFE(estimator=LinearRegression(), n_features_to_select=5)
rfe.fit(X_train,y_train)


# In[38]:


#Columns selected by RFE and their weights
list(zip(X_train.columns,rfe.support_,rfe.ranking_))


# In[39]:


#Function to build a model using statsmodel api
def build_model(cols):
    X_train_sm = sm.add_constant(X_train[cols])
    lm = sm.OLS(y_train, X_train_sm).fit()
    print(lm.summary())
    return lm


# In[40]:


#Function to calculate VIFs and print them -Takes the columns for which VIF to be calcualted as a parameter
def get_vif(cols):
    df1 = X_train[cols]
    vif = pd.DataFrame()
    vif['Features'] = df1.columns
    vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]
    vif['VIF'] = round(vif['VIF'],2)
    print(vif.sort_values(by='VIF',ascending=False))


# In[41]:


#Print Columns selected by RFE. We will manually eliminate for these columns
X_train.columns[rfe.support_]


# In[42]:


# Features not selected by RFE
X_train.columns[~rfe.support_]


# In[43]:


# Taking 15 columns supported by RFE for regression
X_train_rfe = X_train[['yr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'season_spring',
       'season_summer', 'season_winter', 'mnth_jan', 'mnth_jul', 'mnth_sept', 'weekday_sat',
       'weathersit_bad', 'weathersit_moderate']]


# In[44]:


X_train_rfe.shape


# ## Model-1

# In[45]:


# Check data types
print(X_train.dtypes)

# Convert data types (if necessary)
X_train['yr'] = X_train['yr'].astype(int)
X_train['holiday'] = X_train['holiday'].astype(int)
X_train['workingday'] = X_train['workingday'].astype(int)
X_train['temp'] = X_train['temp'].astype(float)
X_train['hum'] = X_train['hum'].astype(float)
X_train['windspeed'] = X_train['windspeed'].astype(float)
X_train['season_spring'] = X_train['season_spring'].astype(int)
X_train['season_summer'] = X_train['season_summer'].astype(int)
X_train['season_winter'] = X_train['season_winter'].astype(int)
X_train['mnth_jan'] = X_train['mnth_jan'].astype(int)
X_train['mnth_jul'] = X_train['mnth_jul'].astype(int)
X_train['mnth_sept'] = X_train['mnth_sept'].astype(int)
X_train['weekday_sat'] = X_train['weekday_sat'].astype(int)
X_train['weathersit_bad'] = X_train['weathersit_bad'].astype(int)
X_train['weathersit_moderate'] = X_train['weathersit_moderate'].astype(int)
X_train['weekday_sun'] = X_train['weekday_sun'].astype(int)
X_train['weathersit_moderate'] = X_train['weathersit_moderate'].astype(int)
# Check for any non-numeric data in the columns
print(X_train.applymap(np.isreal).all())

#Selected columns for Model 1 - all columns selected by RFE
cols = ['yr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'season_spring',
       'season_summer', 'season_winter', 'mnth_jan', 'mnth_jul', 'mnth_sept', 'weekday_sat',
       'weathersit_bad', 'weathersit_moderate']

build_model(cols)


# In[46]:


#Selected columns for Model 1 - all columns selected by RFE
cols = ['yr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'season_spring',
       'season_summer', 'season_winter', 'mnth_jan', 'mnth_jul', 'mnth_sept', 'weekday_sat',
       'weathersit_bad', 'weathersit_moderate']

build_model(cols)
get_vif(cols)


# ### Model-2

# In[47]:


# Dropping the variable mnth_jan as it has negative coefficient and is insignificant as it has high p-value
cols = ['yr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'season_spring',
       'season_summer', 'season_winter', 'mnth_jul', 'mnth_sept', 'weekday_sat',
       'weathersit_bad', 'weathersit_moderate']
build_model(cols)
get_vif(cols)


# ### Model-3

# In[48]:


# Dropping the variable hum as it has negative coefficient and is insignificant as it has high p-value
cols = ['yr', 'holiday', 'workingday', 'temp', 'windspeed', 'season_spring',
       'season_summer', 'season_winter', 'mnth_jul', 'mnth_sept', 'weekday_sat',
       'weathersit_bad', 'weathersit_moderate']
build_model(cols)
get_vif(cols)


# ### Model-4

# In[49]:


# Dropping the variable holiday as it has negative coefficient and is insignificant as it has high p-value
cols = ['yr', 'workingday', 'temp', 'windspeed', 'season_spring',
       'season_summer', 'season_winter', 'mnth_jul', 'mnth_sept', 'weekday_sat',
       'weathersit_bad', 'weathersit_moderate']
build_model(cols)
get_vif(cols)


# ### Model-5

# In[50]:


# Dropping the variable mnth_jul,temp as it has negative coefficient and is insignificant as it has high p-value
cols = ['yr', 'workingday', 'temp', 'windspeed', 'season_spring',
       'season_summer', 'season_winter', 'mnth_sept', 'weekday_sat',
       'weathersit_bad', 'weathersit_moderate']
build_model(cols)
get_vif(cols)


# ### Model-6

# In[51]:


## Trying to replace July with spring as both were highly correlated
#removing of working_day,Summer and adding temp

cols6 = ['yr','season_spring', 'mnth_jul',
        'season_winter', 'mnth_sept', 'weekday_sun',
       'weathersit_bad', 'weathersit_moderate','temp']
build_model(cols6)
get_vif(cols)


# Here VIF seems to be almost accepted. p-value for all the features is almost 0.0 and R2 is 0.82
# Hence we finalize this model to use further

# In[52]:


#Build a model with all columns to select features automatically
def build_model_sk(X,y):
    lr1 = LinearRegression()
    lr1.fit(X,y)
    return lr1


# In[53]:


#Let us build the finalmodel using sklearn
#Build a model with above columns
lr = build_model_sk(X_train[cols6],y_train)
print(lr.intercept_,lr.coef_)


# ### Residual Analysis

# In[54]:


y_train_pred = lr.predict(X_train[cols6])


# In[55]:


#Plot a histogram of the error terms
def plot_res_dist(act, pred):
    sns.distplot(act-pred)
    plt.title('Error Terms')
    plt.xlabel('Errors')


# In[56]:


plot_res_dist(y_train, y_train_pred)


# In[57]:


# Actual vs Predicted
c = [i for i in range(0,len(X_train),1)]
plt.plot(c,y_train, color="blue")
plt.plot(c,y_train_pred, color="red")
plt.suptitle('Actual vs Predicted', fontsize = 15)
plt.xlabel('Index')
plt.ylabel('Demands')
plt.show()


# ## Model Predictions

# ## R-Squared value for train predictions

# In[58]:


#Print R-squared Value
r2_score(y_train,y_train_pred)


# ### Prediction of values on test dataset

# In[59]:


#Scale variables in X_test
num_vars = ['temp','atemp','hum','windspeed']

#Test data to be transformed only, no fitting
X_test[num_vars] = scaler.transform(X_test[num_vars])


# In[60]:


cols6 = ['yr','season_spring', 'mnth_jul',
        'season_winter', 'mnth_sept', 'weekday_sun',
       'weathersit_bad', 'weathersit_moderate','temp']

#Predicting test data values
y_test_pred = lr.predict(X_test[cols6])


# ## R-Squared value for test predictions

# In[61]:


# Find out the R squared value between test and predicted test data sets.  
r2_score(y_test,y_test_pred)


# ## Evaluating the model

# In[62]:


#evaluating the model based on r2_score(as mentioned in the problem statement as well)
y_pred=y_test_pred
r2_score(y_test, y_pred)


# In[63]:


fig = plt.figure()
plt.scatter(y_test, y_pred)
fig.suptitle('y_test vs y_pred', fontsize = 15)             
plt.xlabel('y_test', fontsize = 14)                          
plt.ylabel('y_pred', fontsize = 12) 


# In[64]:


#Let us rebuild the final model of manual + rfe approach using statsmodel to interpret it
cols6 = ['yr','season_spring', 'mnth_jul',
        'season_winter', 'mnth_sept', 'weekday_sun',
       'weathersit_bad', 'weathersit_moderate','temp']

lm = build_model(cols6)


# ### Conclusion
# Significant variables to predict the demand for shared bikes
# 
# - holiday
# - temp
# - hum
# - windspeed
# - Season
# - months(January, July, September, November, December)
# - Year
# - Sunday
# - weathersit( Light Snow, Mist + Cloud
